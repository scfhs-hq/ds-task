{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity data analyst nanodegree- P3\n",
    "\n",
    "### Open Street Map - Data Wrangling\n",
    "\n",
    "### Yasser Alnalhli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "   - OpenStreetMap of Amsterdam City, The Netherlands, has been used.\n",
    "    \n",
    "   - The data has been downloaded as XML format from [mapzen](https://mapzen.com). Thu, Jun 15, 2017.\n",
    "   \n",
    "   <img src=\"AmstbiggerOSM.png\">\n",
    "    \n",
    "   - Only the old center of the city has been selected. It is the most visited area of Amsterdam.\" It is known for its traditional architecture, canals, shopping, and many coffeeshops. Dam Square is considered its ultimate centre, but just as interesting are the areas around Nieuwmarkt and Spui. The Red Light District is also a part of the Old Centre.\" [for more information](http://wikitravel.org/en/Amsterdam/Old_Centre).\n",
    "   \n",
    "   \n",
    "   <img src=\"AmstOSM.png\">\n",
    "   \n",
    "    \n",
    "   - I spent some time between 2012 and 2013 in Amsterdam. I liked the culture there. There are very big differences between Arabic culture and the Dutch...I liked the Old Center the most in Amsterdam. It is like an openair big museum. Many things to see and discover. That is why the Old center was my decision in this project. \n",
    "    \n",
    "   - The data wrangling process of Amsterdam XML OSM has been done by discovering auditing and cleaning the dataset before further analysis using MongoDB.\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Faced: \n",
    "\n",
    " - Becouse of my computer's hardware and the bad internet I have, I decided to use just little above the suggested file size (>50MB). I had to use another internet and computer to download the proper xml file size. It took some time.\n",
    " \n",
    " - Choosing just a small part of the city did not reflect the overall finding of the analysis. So my current work is just for the old center of Amsterdam and might not be the same if I choosed a larger area.\n",
    " \n",
    "  \n",
    " - The Decision to use MySQL or MongoDB also took some time, I spent some time to look at both as I did not have much knowledge about both. \n",
    " \n",
    " - The OSM XML file of Amsterdam is quite nice and clean comparing to the other cities in the world. I saw Chicago's data at Udacity vedios and I checked the files of Riyadh, KSA where I am from, and they are little bit messy. Although Dutch Language is similar to English, but mostly English been used in XML files OSM Amsterdam. This is actually was challenge to decide what do I need to clean in the file and I came up with another idea :) \n",
    " \n",
    " - One of the huge cultural differences I faced when I was in Amsterdam, the homosexiality. Amsterdam is a homosexual friendly and there are many bars or other stuff which I want to locate\n",
    "\n",
    " \n",
    " - Almost in all the world, a coffeeshop is a shop to have a nice coffee. Sometimes it been called Cafe. However, in Amsterdam, a coffeeshop is a drug shop or a shop to legally smoke some cannabis. This misunderstanding will be adressed in future steps.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the downloaded XML file:\n",
    "\n",
    "Loading all the libraries for used in the python code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63300649\n"
     ]
    }
   ],
   "source": [
    "#The size of the xml file\n",
    "osm_file = \"Amsterdam61M.osm\"\n",
    "print (os.path.getsize(osm_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it shown above, it is quite big file to test the codes with. So it is better if a sample file less than 2 MB generated from the original file to be used during the code testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is from file creatSample.py\n",
    "\n",
    "\n",
    "osm_file = \"Amsterdam61M.osm\"\n",
    "SAMPLE_FILE = \"Amst_Sample.osm\"\n",
    "\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every 10th top level element\n",
    "    for i, element in enumerate(get_element(osm_file)):\n",
    "        if i % 50 == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above program is a sample of the original file with about 1.3 MB only. Here is the exact size below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275367\n"
     ]
    }
   ],
   "source": [
    "print (os.path.getsize('Amst_Sample.osm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using iterative parsing to process the xml file to find out what are the tags and how many of them are there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of primary tags:\n",
      "defaultdict(<type 'int'>, {'node': 218600, 'nd': 228655, 'bounds': 1, 'member': 7678, 'tag': 515651, 'osm': 1, 'way': 24823, 'relation': 654})\n"
     ]
    }
   ],
   "source": [
    "# From file count_all.py\n",
    "\n",
    "\n",
    "osm_file = \"Amsterdam61M.osm\"\n",
    "\n",
    "\n",
    "# Define a function to count all types of tags in the xml file\n",
    "def count_tags(filename):\n",
    "    tags_dict = defaultdict(int)\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        tags_dict[elem.tag] += 1\n",
    "    return tags_dict\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    tags = count_tags(osm_file)\n",
    "    print(\"counts of primary tags:\")\n",
    "    pprint.pprint(tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going deeper to the xml file. Looking for only the secondary tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts of secondary tags:\n",
      "771\n"
     ]
    }
   ],
   "source": [
    "#From the file count_secondary_tag.py\n",
    "\n",
    "def count_secondary_tag(filename):\n",
    "        tag_keys={}\n",
    "        # find the count of keys in tags\n",
    "        for event, elem in ET.iterparse(filename):            \n",
    "            if elem.tag == 'tag' and 'k' in elem.attrib:\n",
    "                if elem.get('k') in tag_keys.keys():\n",
    "                    tag_keys[elem.get('k')]=tag_keys[elem.get('k')]+1\n",
    "                else:\n",
    "                    tag_keys[elem.get('k')]=1  \n",
    "        # sort the tag in reverse order\n",
    "        import operator\n",
    "        sorted_keys = sorted(tag_keys.items(), key=operator.itemgetter(1)) \n",
    "        sorted_keys.reverse()    \n",
    "        return sorted_keys    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    # audit/count secondary tag\n",
    "    secondary_tag = count_secondary_tag(osm_file)\n",
    "    print(\"counts of secondary tags:\")\n",
    "    print (len(secondary_tag))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking into the tag types and check if it contains any problems regarding to its key element. This will help to understand the data abit more and check the key validation to the MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 159586, 'lower_colon': 355377, 'other': 688, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "# This is  from file Tag_Type.py\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k_value = element.attrib['k']\n",
    "        if lower.search(k_value) is not None:\n",
    "            keys['lower'] += 1\n",
    "        elif lower_colon.search(k_value) is not None:\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(k_value) is not None:\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = process_map('Amsterdam61M.osm')\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, contributed users only will be investigating more deepely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([])\n",
      "set(['Andyngo', 'beetletun', 'monena41', 'alv', 'OSMF Redaction Account', 'eggie', u'J\\xe9r\\xe9my Bachy', 'RichRico', 'alina udodova', 'xybot', 'Rejo Zenger', 'wimvantklooster', 'Ale Bels', 'outofofficeagain', 'Friendly15', 'pierlux', 'Manu1400', 'Linda_Esperanto', 'Martin2009', 'Spencer Peck', 'BCNorwich', 'stanton', 'mabapla', 'Joost van Os', 'sebastic_BAG', 'hybridOL', u'Beselch Gonzalez Pe\\xf1a', 'bigbug21', 'eugenebata', 'padvinder', 'Spyd7r', 'MA-PH', 'pilotrobert', 'AppAmapper', 'wika-osm', 'Aliossandro', 'rob72', 'W-PH', 'Cieper', 'Nikita Mashkin', 'Brunovanm', 'Vicky Coelho', 'Batum Sanctum', 'beweta', 'Hanno Lans', 'wolfv', '42429', 'LeTopographeFou', 'usaf8', 'Chris Parker', 'morsi', 'GayLinc', 'Denis Kamenshchikov', 'Hendrikklaas', 'mboeringa', 'ImreSamu', 'thebonnetplume', 'A10dep', 'saschahaberkorn', 'woodpeck_repair', 'Blauwdruk', 'optimus_ed', 'Amaroussi', 'Dianne1990', 'randomjunk', 'Lancelot van Duin', 'KnotNuts', 'denilsonsa', 'stvn', '_sev', 'Thibaut75011', 'JuniorFrs', 'look4book', 'vindert', 'uboot', 'Bryce C Nesbitt', 'Andrew de Lisle', 'Bruno Bruto da Costa', 'clara', 'rivw', 'BoomLeslie', 'mfvl', 'LucianHex5', 'TheExplorator', 'o_s_m_apper', 'SkaBook', 'DavidLos', 'twq', 'Chris-W', 'trnstlntk', 'mtenbrinke', 'Anamickabroad', 'dawnbreak', 'marczoutendijk', 'ypid', 'Massinissa Kharfallah', 'Dahee Choe', 'AtonX', 'Pau69', 'Mcke', 'Coucouf', 'WJtW', 'ArjanO', 'arvdk', 'lowercase', 'Lina Kulakova', 'KartoGrapHiti', 'pouchon70', 'Martien Sch', 'Polyglot', 'Giannis mits', 'eugene-lisovskiy', 'Bao Ngo Gia', 'Alexandr-cfif', 'ybensadoun', 'Addiel Lora', 'Franzkater', 'Essex_Boy', 'ethelmermaid', 'Shmias', 'Steven Vance', u'Walter Schl\\xf6gl', 'Raymond', 'otx8a', 'dbaron', 'vvoovv', 'Renato Baeta', 'seird99', 'menzo2003', 'wheelmap_android', 'sim', 'StevenW', 'sicco', 'Jake Strine', 'AnkEric', 'Henkinator023', 'wheelmap_visitor', 'schumyHH', 'StevenF', 'Alter_Ego', 'acracia', 'Paolo Gianfrancesco', 'florisje_BAG', 'Kozlovushek', 'PiHe', 'Tammamo77', 'mueschel', 'Br Kessels', 'Larisa Tokareva', 'ghh', 'praise_jah', 'Achille Talon', 'BrittApril', 'Nullspoon', 'Dutchman1978', 'StevAlba', 'Maarten Deen', 'jutezak', 'Alex Stabinskas', 'Marcello Augusto de Almeida', 'Jeroen de Kwaasteniet', 'nimapper', 'Arcam_Rick', 'j_or_n', 'brbbl', 'XanderN', 'Alex Zambrano', 's-wg', 'rethna', 'ToffeHoff', 'ulfl', 'Stemby', 'Johan Ur Riise', 'Larifari77', u'L\\xe9on', 'gerwitz', 'Stefan de Konink', 'fsteggink', 'Gonzalo_Pires', 'Accessible Amsterdam', 'mdk', 'StellanL', 'silviaem', u'\\u041c\\u0438\\u0445\\u0430\\u0438\\u043bC', 'JoshuaVandenberg', 'JNH milieuadvies', 'Woktogoheadquarter', 'timanovic', 'Sylvain Gagnon', 'vshcherb', 'quibos52', 'Bas de Wit', 'mvexel', 'kbouck', 'Cpl_Uhl', 'Renzillo', 'Cartagena2011', 'sbuiting', 'emvee', 'byckel', 'Math1985', 'dromedar61', 'T-Mac', 'richardbrinkman', 'Arrangy', 'highflyer74', 'SimonA', 'Splodge', 'Pilar Perez', 'specialtours', 'Virgile1994', 'wrouesnel', 'Kachkaev', u'Marcos Tom\\xe1s', 'bervanvliet', 'Speedy494', 'Anni Su', 'Johan Wieland', 'meeuw', 'Trucker88', 'Hilton Hotels', '_Mathieu_', 'Freek Bes', 'Mart van Santen', 'Kayla Nguyen', 'Exprezzz', 'apollinaire', 'swanilli', 'JensKveli', 'florisje', 'Barneys Farm Seeds', 'gbakhuis', 'Ninuse', 'Reinier Battenberg', 'nurdafur', 'Aki Tsutsumizaki', 'Mike Donovan', 'OttoR', 'Kimichen', 'writhe', 'uncycle', 'Marion Barry', 'ZMWandelaar', u'\\xc6var Arnfj\\xf6r\\xf0 Bjarmason', 'hatzfeld', 'eythian', 'Derick Rethans', 'Huig', 'Julio_Costa_Zambelli', 'Victor Omirov', 'BAGgeraar', 'JJJWegdam', 'themiurgo', 'Denis_Helfer', 'Anti', 'RaffyO', 'KooKai1971', 'Osbornec', 'AlaskaDave', 'WKOSM', 'wilkoe', 'Marco Meurs', 'Oleg  Balunenko', \"Luca D'Oro\", 'lffis', 'StijnB', 'CitymapperHQ', 'DLP601', 'thetornado76', 'Roflcopter', 'Stegenwachter', 'freddy2001', 'flierfy', 'Ksenia Amer', 'Sumurai8', 'dvor5525', 'maggot27', 'mackerski', 'Syl', 'Math1985_mechanical', 'tiiiim', 'DeMuiz', 'Aaron Lidman', 'stefanwillmeroth', 'SMillerNL', 'shured', 'Yury Melnichek', 'ligfietser', 'Franz Geiger', 'Local Tourist', 'Thoranin Triwit', 'vialma', 'PaulN1963', 'Sander H', 'Jethro10', 'ijsb', 'Christoph Lotz', 'Husky', 'raaf', 'iftekhar25', 'Jeroen Muris', 'KasperK', 'Kjell-Degroote', 'ToddHB', 'The Maarssen Mapper', 'GarminRob', 'skunk', 'Leah Sparks', 'Artem Lobintsev', 'robin0van0der0vliet', 'edwin_wisse', 'someosmanduser', 'Jeyanthan', 'tixuwuoz', 'Chrisi_LE', 'karitotp', 'glozzie', 'woodpeck', 'CasualMapper', 'Lenicz', 'Heliosmaster', 'Green1984', 'ru-Spider', 'AdVerburg', 'PeterLl', 'jawiwi', 'HHCacher', 'driveland', 'Max Bruinsma', \"It's so funny_mechanical\", 'grossing', 'AEelderink', 'aintlocal', 'Kaligule', 'r4b1d', 'gsfjohndoe', 'Gulontheroad', 'plummm', 'Italian', 'EstherH', 'Simon Gee', u'Micha\\xebl van Eeden', 'Batisteo', 'Oppie', 'Dave330', u'\\u042e\\u043b\\u044f \\u0425\\u043c\\u0435\\u043b\\u044c\\u043d\\u0430\\u044f', 'dannykath', 'webmind', 'jknockaert', 'M!dgard', 'Sander Malschaert', 'Plompy', 'Justabum', 'cruiserpaule', 'ezekielcyrus', 'rullzer', 'mOlind', u'B\\xfc\\u015fra Yi\\u011fit', 'geozeisig', 'virgiotero', 'SomeoneElse_Revert', 'IIVQ', 'osm183945', 'Jedrzej Pelka', 'Commodoortje', 'BugBlue', 'ratrun', 'Lyolya Dmitrieva', 'FrankBouwhuis', 'RubioV', 'milovanderlinden', 'DKDBEM4', 'AND', u'N\\xedcolas Loukides', 'Ogurec', 'unknown365', 'hsimpson', 'bvankempen', 'knutblub', 'rowisp', 'Alexander Limorenko', 'Zead Bazrgan', 'dvdhoven', 'FredB', 'lsteelandt', 'jaimemd', 'vanx0', \"It's so funny\", 'Aury88', 'Peter Koh YC', 'AndriesWijma', 'GeoGrafiker', 'David Mackerle', 'wvdp', 'wouterMe', 'zireael', 'aceman444', 'balducien', 'Declan Wicks', 'Nanofstan', 'Marijn', 'Stereo_repair', 'phicoh', 'mokum events', 'Hvieira83', 'calfarome', 'mapper999', 'Tarka', 'VoetbalTravel', 'thebasics', 'Aleks-Berlin', 'Hans Alexander', 'bjcott', 'Django de Hond', 'Heyheyitshay', 'zufanka', 'Dutch Mapper Mechanical', 'KatyaKholyapina', 'gmcternan', 'HourOfTheWulf', 'kannix', 'soso67', 'Tourepel', 'Frankl2009', 'GercoKees', 'sfosm', 'sebastic', 'Pptje', 'Nikola K', 'atlas moufid', 'Leonmvd', 'thenew', 'Anna_AG', 'hansiwasiheiri', 'jonathamerico', '_al', 'lyctkel', 'Peter Dobratz', 'Remco Zut', 'Zaur523', 'JamesGrace', 'iiizio', 'Andre68', 'gam3', 'Buxtehude', 'cashweaver', '761', 'kiwipferd', 'Josef73', 'Skywave', 'mono11', 'palimpadum', 'Pandule', 'foerno', 'Jobw', 'web', 'Ulmon Community', 'Gilimanjaro', 'Aprillovechild', 'vazooza', 'Ronny Standtke', 'likesrhubarb', 'Bengatzer', 'IByte', 'David Crochet', 'MA-S', 'AliBaba', 'cruz', 'Kira Laktionov', 'Alpin100', 'pizzaiolo', 'Peter Hacke', 'Doxin', 'Nearo', 'Tatyana2016', 'andrewpmk', 'Jacque', 'wwt', 'mfkne', 'plantagenet', 'ekes', 'CycleStreets', 'philip1201', 'WhiskyMacK', u'G\\xfcrkan \\u0130pek', 'Mimeya', 'Med', 'kisaa', 'jenniepuff', 'gvb', 'adjuva', 'mnegrini', 'the Sandinator', 'Sint E7', 'BuganiniQ', 'FvGordon', 'Nico Witteman', 'jkbtb', 'AlexAmster', 'borishag', 'OpenVegeMap', 'Claudius Henrichs', 'Vreeken', 'Chiuaua', 'Danila Futoran', u'Noyan G\\xfcndogdu', 'Gertjan Idema', 'coffeeshop tours', 'Havan Hees', 'DutchMapper', 'Imergis', 'nyuriks', 'kaafka', 'Blobo123', 'shravan91', 'Zandlopertje', 'CJTmmr', 'abel801', 'AndiG88', 'Markus59', 'Rbanick', 'jules16v', 'Sebaochoa', 'Marcel-NL', 'Zverik', 'HanW', 'edhelas', 'Sven Witte', 'Sybren', 'Gijsrooy', 'Bram P', 'slijkhuis', 'Thomas Wielemaker', 'AxelBoldt', 'Michiel M', 'textlijn', 'Maxim  Morgaev', 'xyzl', '3dShapes', 'ChrissW-R1', 'paulbe', 'HolgerJ', 'LHCarsten', 'CoreyFarwell', 'ger4rd', 'Jegroen', 'MapperIAm', 'VinceLoi', 'Freek', 'JuliaShi', 'ErikAms', 'W-S', 'AND_fixbot', 'Pemberton', 'Skidle', u'N\\xf3s', 'bibi6', 'Thor58', 'morray', 'Larisa Golub', 'ff5722', 'Tor Bendiksen', 'Jan Westerhof', 'Kerstkonijn', 'marczoutendijk_mechanical', 'bmichel', 'stroet43', 'Gutsycat', 'Niels Elgaard Larsen', 'dsbaars', 'riad', 'Emilien', 'rolandmwagner', 'Digne', 'Coinpluggin', 'aartjanszen', 'Maarten van der Veen', 'jonathanrcarter', 'LivingWithDragons', 'jakednb', 'elpbatista', u'Zugf\\xfchrer', 'anneb', 'Dennis Fargey', 'martinum4', 'Greenbudgetbikes', 'Meeuw', 'Valentijn', 'HenkL', 'jsoetendal', 'timmersen', 'tilsti', 'Arjan Pals', 'Ashley D'])\n",
      "594\n"
     ]
    }
   ],
   "source": [
    "# this code is from the file Users.py\n",
    "\n",
    "def get_user(element):\n",
    "    return element.get(\"user\")\n",
    "\n",
    "#find and count\n",
    "def count_unique(filename):\n",
    "    users = set()\n",
    "    print(users)\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if \"user\" in element.attrib:\n",
    "            users.add(get_user(element))\n",
    "    print(users)\n",
    "    print(len(users))\n",
    "\n",
    " \n",
    "count_unique(osm_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look more deep into the xml file and find things:\n",
    "\n",
    "As I mentioned above, one main problem I faced when hanging out in Amsterdam with my wife, is sometime going to homosexial bar or pub which is fine, but it took me long time to distingush them from other normal cafes. It is just a cultural differences. Espically to my Muslim wife with Hijab. Many funny stories I have, but I will keep it to myself. \n",
    "\n",
    "So, I decided to first count how many \"gay\" places in that particular area. Then, What are thier names and location. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38 gay friendly place at the old center of Amsterdam\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "To find how many gay frindly place in Amsterdam's old center, \n",
    "I look into the XML file and make this function to count them 1st.\n",
    "\n",
    " This is from countGay.py File \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "osm_file = \"Amsterdam61M.osm\"\n",
    "count = 0\n",
    "\n",
    "\n",
    "def find_gay(osmfile):\n",
    "\tcount = 0\n",
    "\t#parse the file\n",
    "\tfor event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\t\t#check both ways and node tags\n",
    "\t\tif elem.tag == \"node\" or elem.tag == \"way\":\n",
    "\t\t\t#check the key at the tag\n",
    "\t\t\tfor tag in elem.iter(\"tag\"):\n",
    "\t\t\t\t#check the condition\n",
    "\t\t\t\tif tag.attrib['k'] == \"gay\" and  tag.attrib['v'] == \"yes\":\n",
    "\t\t\t\t\t#print(tag.attrib['k'] + \" friendly place number: \" + str(count))\n",
    "\t\t\t\t\tcount += 1\n",
    "\tprint (\"There are \" + str(count) + \" gay friendly place at the old center of Amsterdam\")\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "\n",
    "find_gay(osm_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IHLIA Homodok\n",
      "lat: 52.3762463, lon: 4.9081983\n",
      "De Barderij\n",
      "lat: 52.3758329, lon: 4.9004544\n",
      "Mankind\n",
      "lat: 52.3607475, lon: 4.888122\n",
      "Café Rouge\n",
      "lat: 52.3669068, lon: 4.8961508\n",
      "Pink Point\n",
      "lat: 52.3741824, lon: 4.8844544\n",
      "Eagle\n",
      "lat: 52.3746491, lon: 4.897024\n",
      "Vrolijk\n",
      "lat: 52.3722426, lon: 4.8898085\n",
      "Getto\n",
      "lat: 52.375358, lon: 4.898286\n",
      "Vivelavie\n",
      "lat: 52.3663271, lon: 4.8980529\n",
      "Saarein\n",
      "lat: 52.370287, lon: 4.8802861\n",
      "Amstel 54\n",
      "lat: 52.3668687, lon: 4.895663\n",
      "HotSpot\n",
      "lat: 52.366956, lon: 4.8968557\n",
      "Fame\n",
      "lat: 52.3668819, lon: 4.8954738\n",
      "Gollem\n",
      "lat: 52.3660948, lon: 4.8999054\n",
      "Montmartre\n",
      "lat: 52.366561, lon: 4.895936\n",
      "Music Box\n",
      "lat: 52.3666392, lon: 4.8983205\n",
      "Entre Nous\n",
      "lat: 52.3665536, lon: 4.8957327\n",
      "Reve Museum\n",
      "lat: 52.3759303, lon: 4.9084423\n",
      "'t Mandje\n",
      "lat: 52.374815, lon: 4.900983\n",
      "Motor Sportclub Amsterdam\n",
      "lat: 52.3763379, lon: 4.9022053\n",
      "Sultana\n",
      "lat: 52.3666507, lon: 4.8899875\n",
      "Downtown\n",
      "lat: 52.3665466, lon: 4.8903128\n",
      "Ludwig\n",
      "lat: 52.3664942, lon: 4.8906805\n",
      "Soho\n",
      "lat: 52.3662911, lon: 4.8909603\n",
      "Taboo\n",
      "lat: 52.3664491, lon: 4.8909913\n",
      "Reality\n",
      "lat: 52.3658704, lon: 4.8952407\n",
      "ProGay\n",
      "lat: 52.3659111, lon: 4.8949754\n",
      "'t Dwarsliggertje\n",
      "lat: 52.3660235, lon: 4.8941948\n",
      "The Web\n",
      "lat: 52.3765096, lon: 4.8957377\n",
      "Cuckoo's Nest\n",
      "lat: 52.3763554, lon: 4.8949926\n",
      "Dirty Dick's\n",
      "lat: 52.3747299, lon: 4.8971308\n",
      "Mr. B\n",
      "lat: 52.374644, lon: 4.8972334\n",
      "RoB\n",
      "lat: 52.3749991, lon: 4.8977663\n",
      "Lellebel\n",
      "lat: 52.3656807, lon: 4.897438\n",
      "Prik\n",
      "lat: 52.3747124, lon: 4.8908445\n",
      "Club Church\n",
      "lat: 52.3652055, lon: 4.8856002\n",
      "Homomonument\n"
     ]
    }
   ],
   "source": [
    "\"\"\"NOw I will check the name of that place and the location\n",
    "# for this, I used FindGay.py\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "osm_file = \"Amsterdam61M.osm\"\n",
    "count = 1\n",
    "\n",
    "\n",
    "def find_gay(osmfile):\n",
    "\tcount = 1\n",
    "\tfor event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\t\tif elem.tag == \"node\" or elem.tag == \"way\":\n",
    "\t\t\tfound_gay = False\n",
    "\t\t\tfor tag in elem.iter(\"tag\"):\n",
    "\t\t\t\tif tag.attrib['k'] == \"gay\" and  tag.attrib['v'] == \"yes\":\n",
    "\t\t\t\t\tcount += 1\n",
    "\t\t\t\t\tfound_gay = True\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tif found_gay == True:\n",
    "\n",
    "\t\t\t\t# we have found the node containing gay tag, so lets print the name\n",
    "\t\t\t\tfor tag in elem.iter(\"tag\"):\n",
    "\t\t\t\t\tif tag.attrib['k'] == \"name\":\n",
    "\t\t\t\t\t\tprint(tag.attrib['v'])\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\t# also lets print the lat and long values present in the node \n",
    "\t\t\t\t# remember that we don't have lat, lon in way nodes\n",
    "\t\t\t\tif elem.tag == 'node':\n",
    "\t\t\t\t\tprint(\"lat: \" + elem.attrib['lat'] + \", lon: \" + elem.attrib['lon'])\n",
    "\t\t\t\t\n",
    "\n",
    "find_gay(osm_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem that many tourists face, is the \"Coffeeshops\" in Amsterdam. Dutch call a Coffeeshop for drug shop where you can legally smoke weed, cannabis or other drugs. I will audit this somehow to make things clearer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"this code is from the file Audit_coffeeshops.py\n",
    "\n",
    "\n",
    "# refer to the link for medhods used\n",
    "# https://docs.python.org/2/library/xml.etree.elementtree.html\"\"\"\n",
    "\n",
    "def update_save(oldfile, newfile):\n",
    "    tree = ET.parse(oldfile)\n",
    "    root = tree.getroot()\n",
    "\n",
    "\n",
    "    for tag in root.iter('tag'):\n",
    "        #here we have all the tag elements\n",
    "        # check for the key = shop and v contaings drug\n",
    "        if tag.attrib['k'] == 'shop' and 'drug' in tag.attrib['v']:\n",
    "            # update the value to drug store\n",
    "            tag.set('v', 'drug store')\n",
    "        # if the v == coffee_shop and k = cuisine\n",
    "        elif tag.attrib['v'] == 'coffee_shop' and tag.attrib['k'] == 'cuisine':\n",
    "            # update the value to drug store\n",
    "            tag.set('v', 'drug store')\n",
    "    \n",
    "    tree.write(newfile,encoding=\"UTF-8\", xml_declaration=True, default_namespace=None, method=\"xml\")\n",
    "\n",
    "update_save('Amsterdam61M.osm', 'updatedAmst.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem found that another calissifiacation of (soft_drugs) in both cuisine and shop. That also need to be changed and then all shops and cuisines will be unified as the new suggested name (drug store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# THis is from code Soft_Drug_Audit.py \n",
    "\n",
    "\n",
    "def update_save(oldfile, newfile):\n",
    "    tree = ET.parse(oldfile)\n",
    "    root = tree.getroot()\n",
    "\n",
    "\n",
    "    for tag in root.iter('tag'):\n",
    "        #here we have all the tag elements\n",
    "        # check for the value if it contains soft_drugs\n",
    "        if tag.attrib['k'] == 'shop' and tag.attrib['v'] == \"soft_drugs\":\n",
    "            # update the value to drug store\n",
    "            tag.set('v', 'drug store')\n",
    "        # if the v == soft_drugs and k = cuisine\n",
    "        elif tag.attrib['v'] == 'soft_drugs' and tag.attrib['k'] == 'cuisine':\n",
    "            # update the value to drug store\n",
    "            tag.set('v', 'drug store')\n",
    "    \n",
    "    tree.write(newfile,encoding=\"UTF-8\", xml_declaration=True, default_namespace=None, method=\"xml\")\n",
    "\n",
    "update_save('updatedAmst.osm', 'updatedAmst2.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated file cleaned and organized also contains relaiable information at least for me and some tuorists, unlike Dutchs. So it will be used in furhter investigation with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM data exploration using MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First of all, the osm xml will be converted to json format and then inserted to the MongoDB. I used this code from (http://napitupulu-jon.appspot.com/posts/wrangling-openstreetmap.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': {'changeset': '47179797',\n",
      "             'timestamp': '2017-03-26T16:54:07Z',\n",
      "             'uid': '2154858',\n",
      "             'user': 'mboeringa',\n",
      "             'version': '6'},\n",
      " 'id': '26585151',\n",
      " 'pos': [52.3773401, 4.9121287],\n",
      " 'type': 'node'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This code is from the file insert_osm_data.py\n",
    "\n",
    "# source : http://napitupulu-jon.appspot.com/posts/wrangling-openstreetmap.html \"\"\"\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "addresschars = re.compile(r'addr:(\\w+)')\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "osm_file = 'updatedAmst.osm'\n",
    "\n",
    "def shape_element(element):\n",
    "    #node = defaultdict(set)\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        #create the dictionary based on exaclty the value in element attribute.\n",
    "        node = {'created':{}, 'type':element.tag}\n",
    "        for k in element.attrib:\n",
    "            try:\n",
    "                v = element.attrib[k]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            if k == 'lat' or k == 'lon':\n",
    "                continue\n",
    "            if k in CREATED:\n",
    "                node['created'][k] = v\n",
    "            else:\n",
    "                node[k] = v\n",
    "        try:\n",
    "            node['pos']=[float(element.attrib['lat']),float(element.attrib['lon'])]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        if 'address' not in node.keys():\n",
    "            node['address'] = {}\n",
    "        #Iterate the content of the tag\n",
    "        for stag in element.iter('tag'):\n",
    "            #Init the dictionry\n",
    "\n",
    "            k = stag.attrib['k']\n",
    "            v = stag.attrib['v']\n",
    "            #Checking if indeed prefix with 'addr' and no ':' afterwards\n",
    "            if k.startswith('addr:'):\n",
    "                if len(k.split(':')) == 2:\n",
    "                    content = addresschars.search(k)\n",
    "                    if content:\n",
    "                        node['address'][content.group(1)] = v\n",
    "            else:\n",
    "                node[k]=v\n",
    "        if not node['address']:\n",
    "            node.pop('address',None)\n",
    "        #Special case when the tag == way,  scrap all the nd key\n",
    "        if element.tag == \"way\":\n",
    "            node['node_refs'] = []\n",
    "            for nd in element.iter('nd'):\n",
    "                node['node_refs'].append(nd.attrib['ref'])\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    \"\"\"\n",
    "    Process the osm file to json file to be \n",
    "    prepared for input file to mongodb\n",
    "    \"\"\"\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "data = process_map(osm_file)\n",
    "pprint.pprint(data[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is just a sample of the new file. it is json format where we can easiely insert it to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaser/anaconda3/envs/Udacity/lib/python2.7/site-packages/ipykernel/__main__.py:8: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'AmsOSM'), u'AmsMAP')\n"
     ]
    }
   ],
   "source": [
    "# This code is from the file insert_osm_data.py\n",
    "\n",
    "db_name = 'AmsOSM'\n",
    "# Connect to Mongo DB\n",
    "client = MongoClient('localhost:27017')  \n",
    "db = client[db_name]  \n",
    "c = db.AmsMAP\n",
    "c.insert(data)\n",
    "pprint.pprint(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, some analysis of the generated json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The number of data', 2341746)\n",
      "('The number of ways', 238796)\n",
      "('The number of nodes', 2102932)\n",
      "('The number of bicycle parkings', 1006)\n",
      "('The number of tourism attractions', 597)\n"
     ]
    }
   ],
   "source": [
    "# This code is from the file insert_osm_data.py\n",
    "\n",
    "\n",
    "# the number of data\n",
    "print('The number of data',db.AmsMAP.count()) \n",
    "\n",
    "# the Numbers of way\n",
    "print ('The number of ways',db.AmsMAP.find({'type':'way'}).count())  \n",
    "\n",
    "# the number of nodes\n",
    "print ('The number of nodes',db.AmsMAP.find({'type':'node'}).count()) \n",
    "\n",
    "# how many bicycle parkings in Amsterdam old center\n",
    "print ('The number of bicycle parkings',db.AmsMAP.find({'amenity':'bicycle_parking'}).count())\n",
    "\n",
    "\n",
    "# how many tourism attractions in Amsterdam old center \n",
    "print ('The number of tourism attractions',db.AmsMAP.find({'tourism':'attraction'}).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking more deeply into the data with MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 cuisine:\n",
      "[{u'_id': u'italian', u'count': 549},\n",
      " {u'_id': u'burger', u'count': 296},\n",
      " {u'_id': u'thai', u'count': 229},\n",
      " {u'_id': u'regional', u'count': 207},\n",
      " {u'_id': u'indian', u'count': 202}]\n"
     ]
    }
   ],
   "source": [
    "# This code is from the file insert_osm_data.py\n",
    "\n",
    "\n",
    "cuisine = db.AmsMAP.aggregate([\n",
    "        {\"$match\" : {\"cuisine\" : {\"$exists\" : 1}}},\n",
    "        {\"$group\" : {\"_id\" : \"$cuisine\",\n",
    "                     \"count\" : {\"$sum\" : 1}}},\n",
    "        {\"$sort\" : {\"count\" : -1}},\n",
    "        {\"$limit\" : 5}\n",
    "    ])\n",
    "print ('The top 5 cuisine:') \n",
    "pprint.pprint([doc for doc in cuisine])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more about the most common amenity (top 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 common amrity:\n",
      "[{u'_id': u'restaurant', u'count': 5144},\n",
      " {u'_id': u'cafe', u'count': 2368},\n",
      " {u'_id': u'pub', u'count': 1916},\n",
      " {u'_id': u'fast_food', u'count': 1759},\n",
      " {u'_id': u'bench', u'count': 1026}]\n"
     ]
    }
   ],
   "source": [
    "# This code is from the file insert_osm_data.py\n",
    "\n",
    "amenity = db.AmsMAP.aggregate([ \n",
    "                { \"$group\" : { \"_id\" : \"$amenity\",\"count\": {\"$sum\": 1 }}},\n",
    "                { \"$sort\" : { \"count\" : -1 }},\n",
    "                { \"$skip\" : 1 },\n",
    "                { \"$limit\" : 5 }\n",
    "               ])\n",
    "print ('top 5 common amrity:') \n",
    "pprint.pprint([doc for doc in amenity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more to check the popular shops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 common shops:\n",
      "[{u'_id': u'clothes', u'count': 2527},\n",
      " {u'_id': u'gift', u'count': 1074},\n",
      " {u'_id': u'shoes', u'count': 630},\n",
      " {u'_id': u'jewelry', u'count': 458},\n",
      " {u'_id': u'supermarket', u'count': 431},\n",
      " {u'_id': u'convenience', u'count': 373},\n",
      " {u'_id': u'bakery', u'count': 324},\n",
      " {u'_id': u'books', u'count': 306},\n",
      " {u'_id': u'hairdresser', u'count': 279},\n",
      " {u'_id': u'alcohol', u'count': 265}]\n"
     ]
    }
   ],
   "source": [
    "## This code is from the file insert_osm_data.py\n",
    "\n",
    "shops = db.AmsMAP.aggregate([\n",
    "        {\"$match\" : {\"shop\" : {\"$exists\" : 1}}},\n",
    "        {\"$group\" : {\"_id\" : \"$shop\",\n",
    "                     \"count\" : {\"$sum\" : 1}}},\n",
    "        {\"$sort\" : {\"count\" : -1}},\n",
    "        {\"$limit\" : 10}\n",
    "    ])\n",
    "print ('The top 10 common shops:') \n",
    "pprint.pprint([doc for doc in shops])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I was in Amsterdam, I also found it intersted that University of Amsterdam has many locations. I will check to this  dataset and see what I will get:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Universities in Amsterdam:\n",
      "[{u'_id': u'Oudemanhuispoort (UvA)', u'count': 40},\n",
      " {u'_id': u'Amstelcampus', u'count': 9},\n",
      " {u'_id': u'UvA Binnengasthuisterrein', u'count': 9},\n",
      " {u'_id': u'UvA Bungehuis', u'count': 9},\n",
      " {u'_id': u'UvA PC Hoofthuis', u'count': 9},\n",
      " {u'_id': u'Service & Informatiecentrum UvA', u'count': 9},\n",
      " {u'_id': u'UvA BG5', u'count': 9},\n",
      " {u'_id': u'P', u'count': 9},\n",
      " {u'_id': u'Universiteit van Amsterdam', u'count': 9},\n",
      " {u'_id': u'Aula UvA', u'count': 9},\n",
      " {u'_id': u'Ruimtelijke Wetenschappen (FMG)', u'count': 9},\n",
      " {u'_id': u'UvA', u'count': 9},\n",
      " {u'_id': u'UvA Faculteit der Maatschappij-en Gedragswetenschappen',\n",
      "  u'count': 9},\n",
      " {u'_id': u'Spui25', u'count': 9},\n",
      " {u'_id': u'UvA Faculteit der Economische Wetenschappen en Econometrie',\n",
      "  u'count': 9},\n",
      " {u'_id': u'Kohnstammhuis', u'count': 9},\n",
      " {u'_id': u'Theo Thijssenhuis', u'count': 9},\n",
      " {u'_id': u'Roeterseiland (UvA)', u'count': 9}]\n"
     ]
    }
   ],
   "source": [
    "## This code is from the file insert_osm_data.py\n",
    "\n",
    "\n",
    "#University\n",
    "Uni = db.AmsMAP.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\": \"university\", \"name\":{\"$exists\":1}}},\n",
    "            {\"$group\":{\"_id\":\"$name\", \"count\":{\"$sum\":1}}},\n",
    "            {\"$sort\":{\"count\":-1}}])\n",
    "print ('The Universities in Amsterdam:') \n",
    "pprint.pprint([doc for doc in Uni])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What else can be done to the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Amsterdam OSM dataset is very nice and tidy. Many things can be studied and analysies.\n",
    " \n",
    " - Amsterdam is a bicycle friendly and it is the most papular city with biycles in the world. It will be more inteasted if I checked the ways which contains biycles paths. \n",
    " \n",
    " - It is also nice to check the number of places which contains wheelchair access and which dont. The amsterdam city councel or Gementa as the Dutch call it, can use such information to improve the quality of life for those who need such service.\n",
    " \n",
    " - Bars and pubs can be also checked and find what is the most papular.\n",
    " \n",
    " - Amsterdam is also has many museums. I noticed that osm contain many of them. That might be a good choice to check.\n",
    " \n",
    " - Amsterdam's old center has alot of canals. Canals has been build very nicly. The data should contain at least the name of that canal or as the Dutch call Gracht. Gracht is Amsterdam like streets where Amsterdammers and turists enry transporting and moving around the city for site seeing and having fund. \n",
    " \n",
    " - Also Amsterdam is also famous of open air free markets. I did not have a look deeply into the current dataset. but it would be great if they are included and then studied and analysed. \n",
    " \n",
    " - The [mapzen](https://mapzen.com) where I downloaded the dataset, contains separated geojson (Datasets grouped into individual layers by OpenStreetMap tags (IMPOSM)) and there is an importer called [Imposm](https://imposm.org/docs/imposm/latest/) which can be used. We can insert the fils directly to MongoDB nd do the analysis. \n",
    " \n",
    " - I downloaded all those files and you can have a lok into it in a separated file with the submission.\n",
    " \n",
    " - If somone inteasted, there are other ways to manipulate the OSM data  I will mention just few of them here:\n",
    "\n",
    "     - [Overpass API](http://wiki.openstreetmap.org/wiki/Overpass_API)\n",
    "     - [Node-Mongosm](https://github.com/sammerry/node-mongosm)\n",
    "     - [overpy 0.4](https://pypi.python.org/pypi/overpy/)\n",
    "     \n",
    "   and many others. \n",
    "   \n",
    "   (I will add more ideas as suggested from the 1st reviewer.) \n",
    "   \n",
    "  - OSM data can be be fixed or improved to be interactive with the users. A mobile appliction can be a solution for this. Via this application, the user can change directly to the file and add new information, discribtion or rating a particular place. This way will improve and enhance the dataset very fast. \n",
    "  \n",
    "  - However, one main disadvantage of implementing the above idea is the quality of the added information. Also how relaiable is the added information and how usefult it is.\n",
    "  \n",
    "  - Another idea is link it with google maps. However, what I dont like about google map is all owned by google and that might affect OSM as open source and then it can be used in advertisments.\n",
    "  \n",
    "  - We can link OSM somehow to wikipedia XML for more information and discribtions. MongoDB is usefult in this concept.\n",
    "  \n",
    "  - The social media plays a significant role in all internet usage. Linking OSM to th social media espically to twitter might help in different aspects [example1](https://www.theatlantic.com/technology/archive/2013/11/how-online-mapmakers-are-helping-the-red-cross-save-lives-in-the-philippines/281366/) and [example2](https://www.newscientist.com/article/dn24565-social-media-helps-aid-efforts-after-typhoon-haiyan#.U-QaA2MmUro).\n",
    "  \n",
    "  - The above are very nice examples in using OSM. However, we should encorage the users to do such great work by allocating prices and nice compitions for the best practice. \n",
    "  \n",
    "  - The current dataset contains many secondary tags. Those can be summerised and grouped. However, we might loose some details which might be important for someusers to answer a  particular question.\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This kind of data format has alot of advantages. It is clear from the analysis above that many valuable information can be obtained and extracted. The cleaning process was also informative and easy to get a particular kind of information. Moreover, the auditing is another process to handel such information and change many information in easy way. Finally, there are many other ideas can be done in this king of dataset and further analysiss needed to cover them and to adress some issues to improve the quality of the work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    " \n",
    " - Udacity Data Wrangling with MongoDB - Exercises\n",
    " http://fch808.github.io/Data%20Wrangling%20with%20MongoDB%20-%20Exercises.html\n",
    " \n",
    " - http://napitupulu-jon.appspot.com/posts/wrangling-openstreetmap.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
