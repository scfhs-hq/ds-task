{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/scfhs-logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "> This notebook covers the following:\n",
    "* [Analysis Introduction and Objective](#first-bullet)\n",
    "* [Data Collection](#second-bullet)\n",
    "* [Data Preprocessing and Exploration](#third-bullet)\n",
    "* [Modeling](#fourth-bullet)\n",
    "* [Results](#fifth-bullet)\n",
    "* [Used Resources](#sixth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Introduction and Objective  <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "As there is life and joy, there is also death and sadness. For those who lost their loved ones; my scincer condulances.<br> \n",
    "   \n",
    "The objective of this analysis is to accept or reject Andrew's friend hypothesis  which states that \"Some in the Muslim community think people tend to die more (often) in the month of Shaban.\"<br>\n",
    "\n",
    "Using the from\" as part of the pre-interview challenge for the Data Sceitist position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection  <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
    "To prsue this challenge we are going to use the data from [Madinah Municipality](https://services.amana-md.gov.sa/eservicesite/Inq/DeathInquiry.aspx).<br> \n",
    "\n",
    "We are going to collect the death data from 1435-01-01 to 1435-12-30 which is for a five year starting from 1435 till 1439. \n",
    "\n",
    "\n",
    "Unfortunately their website the seem to not have an API to gather the data which would had made things a lot easier. Thus we are going to scrape it from their website ourselfs, and for this we are going to use a python libirary caleld [Selenium](https://selenium-python.readthedocs.io/).<br>\n",
    "\n",
    "Selenium is a powerfull automation tool that will allow us to automate the page navigation to scrape the inqury results table.\n",
    "\n",
    ">**Note:**\n",
    "    Please keep in mind that we are going to assume that this sample data represents the total Muslim population. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing the necessary libraries that we going to use in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define **set_window_size()** function which will be used to change the dropdown values of the inquiry form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_window_size(driver, f_year, f_month, f_day,t_year, t_month, t_day):\n",
    "    '''\n",
    "    Takes the year, month and day values for both from and to dropdown items and sets their value.\n",
    "    Input: webdriver, year, month and day.\n",
    "    Output: no return, changes the dropdown values according to input values. \n",
    "    '''\n",
    "    # From \n",
    "    from_year=Select(driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_cboYFrom\"]')[0])\n",
    "    from_year.select_by_visible_text(f_year)\n",
    "    from_month=Select(driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_cboMFrom\"]')[0])\n",
    "    from_month.select_by_visible_text(f_month)\n",
    "    from_day=Select(driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_cboDFrom\"]')[0])\n",
    "    from_day.select_by_visible_text(f_day)\n",
    "    # To \n",
    "    to_year=Select(driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_cboYTo\"]')[0])\n",
    "    to_year.select_by_visible_text(t_year)\n",
    "    to_month=Select(driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_cboMTo\"]')[0])\n",
    "    to_month.select_by_visible_text(t_month)  \n",
    "    to_day=Select(driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_cboDTo\"]')[0])\n",
    "    to_day.select_by_visible_text(t_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also going to need define **find()** method which will check if an element exists or not and will be used in waiting for element creation to avoid Stale Element Reference exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(driver):\n",
    "    '''\n",
    "    Takes a webdriver and checks if it exists or not. If it does, return element, if not return false\n",
    "    Input: webdriver\n",
    "    Output: Loaded element\n",
    "    '''\n",
    "    element = driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[1]/td/a')\n",
    "    if element:\n",
    "        return element\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now open **Chrome** browser, get the Madinah Municipality website and set the windows size for ouor query to be from 1435-01-01 to 1435-12-30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up a Chrome browser and navigate to web page.\n",
    "driver = webdriver.Chrome() \n",
    "driver.get(\"https://services.amana-md.gov.sa/eservicesite/Inq/DeathInquiry.aspx\")\n",
    "\n",
    "# Set data window size to be from 1435-01-01 to 1435-12-30\n",
    "set_window_size(driver, '1435', '01', '01', '1439', '12', '30')\n",
    "\n",
    "# Click on inquiry button \n",
    "inquiry_button = driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_btnSubmit\"]')[0]\n",
    "inquiry_button.click()\n",
    "\n",
    "# Load pages links\n",
    "page_links = driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[1]/td/a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell below will loop throgh the result pages and start scraping and saving the table data into a csv file called Death_inquiry_from_1435_to_1439. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart = True\n",
    "first_time = True\n",
    "table_data = []\n",
    "temp_row=[]\n",
    "\n",
    "while restart:\n",
    "    for i in range(len(page_links)):# Loop through pages \n",
    "        \n",
    "        # Parse the result table and append it to table_data list to write it into a csv file\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        table_id=\"ctl00_ContentPlaceHolder1_dgDeath\"\n",
    "        for table in soup.findAll('table',{\"id\":table_id}):\n",
    "            \n",
    "            for tr in table.findAll('tr'): \n",
    "                    row = [td for td in tr.stripped_strings]\n",
    "                    if not len(row)> 5 and 'تاريخ الدفن' not in row: # Skip page numbering and column text\n",
    "                        table_data.append(row)\n",
    "                    \n",
    "                    \n",
    "        # Wait for element creation to avoid Stale Element Reference exception\n",
    "        element = WebDriverWait(driver, 10).until(find)\n",
    "        try:\n",
    "            # Click on the '...' to load the next batch of pages \n",
    "            if i == len(page_links)-1 and first_time:# the '...' xpath is different first time\n",
    "                # Write data to file and clear the list for the next batch pages content \n",
    "                with open('Death_inquiry_from_1435_to_1439.csv', 'a', encoding=\"utf-8\") as csvFile:\n",
    "                    for row in table_data:\n",
    "                        writer = csv.writer(csvFile)\n",
    "                        writer.writerow(row)\n",
    "                    csvFile.close()\n",
    "                    table_data=[]\n",
    "                try:\n",
    "                    driver.find_element_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[1]/td/a[20]').click()\n",
    "                except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "                    print('This element no more exists')\n",
    "                first_time=False \n",
    "            elif i == len(page_links)-1 and not first_time:\n",
    "                # Write data to file and clear the list for the next batch pages content \n",
    "                with open('table_data.csv', 'a', encoding=\"utf-8\") as csvFile:\n",
    "                    for row in table_data:\n",
    "                        writer = csv.writer(csvFile)\n",
    "                        writer.writerow(row)\n",
    "                    csvFile.close()\n",
    "                    table_data=[]\n",
    "                try:\n",
    "                    driver.find_element_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[1]/td/a[21]').click()\n",
    "                except NoSuchElementException:  \n",
    "                    print('This element no more exists')\n",
    "            elif not element[i].text == '...':\n",
    "                # Write data to file and clear the list for the next batch pages content \n",
    "                with open('table_data.csv', 'a', encoding=\"utf-8\") as csvFile:\n",
    "                    for row in table_data:\n",
    "                        writer = csv.writer(csvFile)\n",
    "                        writer.writerow(row)\n",
    "                    csvFile.close()\n",
    "                    table_data=[]\n",
    "                    # Move to next page \n",
    "                    element[i].click()\n",
    "            else:\n",
    "                continue\n",
    "        except TimeoutException as ex:\n",
    "            print('Time out exception, waiting for 10 seconds')\n",
    "            time.sleep(10)\n",
    "            pass\n",
    "        \n",
    "    # Load pages links\n",
    "    page_links = driver.find_elements_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[1]/td/a')\n",
    "     \n",
    "    try:\n",
    "        if not first_time:\n",
    "            # if there are no more pages represented by the '...' symbol, stop the while loop \n",
    "            elem = driver.find_element_by_xpath('//*[@id=\"ctl00_ContentPlaceHolder1_dgDeath\"]/tbody/tr[1]/td/a[21]')\n",
    "    except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "        print('No more pages!')\n",
    "        restart = False\n",
    "\n",
    "        \n",
    "# Close the driver \n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
